{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above , we have 50000 images for train set and 10000 images for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       [4]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train is a 2D array, for our classification having 1D array is good enough. so we will convert this to now 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, 4, 1], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.reshape(-1,)\n",
    "y_test[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(X, y, index):\n",
    "    plt.figure(figsize = (15,1))\n",
    "    plt.imshow(X[index])\n",
    "    plt.xlabel(classes[y[index]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(x_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build simple artificial neural network for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.8627 - accuracy: 0.3340\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.6577 - accuracy: 0.4119\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 140s 90ms/step - loss: 1.5674 - accuracy: 0.4472\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 138s 89ms/step - loss: 1.5065 - accuracy: 0.4686\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 143s 92ms/step - loss: 1.4563 - accuracy: 0.4861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2c65651e90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = models.Sequential([\n",
    "        layers.Flatten(input_shape=(32,32,3)),\n",
    "        layers.Dense(3000, activation='relu'),\n",
    "        layers.Dense(1000, activation='relu'),\n",
    "        layers.Dense(10, activation='sigmoid')    \n",
    "    ])\n",
    "ann.compile(optimizer = 'SGD',\n",
    "           loss = 'sparse_categorical_crossentropy',\n",
    "           metrics = ['accuracy'])\n",
    "\n",
    "ann.fit(x_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy can be seen to be 48.61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.66      0.54      1000\n",
      "           1       0.61      0.60      0.60      1000\n",
      "           2       0.33      0.40      0.36      1000\n",
      "           3       0.34      0.37      0.36      1000\n",
      "           4       0.42      0.38      0.39      1000\n",
      "           5       0.54      0.22      0.31      1000\n",
      "           6       0.44      0.68      0.53      1000\n",
      "           7       0.66      0.42      0.52      1000\n",
      "           8       0.61      0.62      0.62      1000\n",
      "           9       0.60      0.48      0.53      1000\n",
      "\n",
      "    accuracy                           0.48     10000\n",
      "   macro avg       0.50      0.48      0.48     10000\n",
      "weighted avg       0.50      0.48      0.48     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_pred = ann.predict(x_test)\n",
    "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
    "\n",
    "print(\"Classification Report :\\n\", classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us build a convolutional neural network to train our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters = 32, kernel_size = (3,3), activation ='relu',input_shape = (32,32,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 1.4616 - accuracy: 0.4719\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 1.1070 - accuracy: 0.6083\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.9804 - accuracy: 0.6570\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.9000 - accuracy: 0.6867\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.8328 - accuracy: 0.7102\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.7811 - accuracy: 0.7277\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 63s 41ms/step - loss: 0.7330 - accuracy: 0.7448\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.6899 - accuracy: 0.7612\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.6579 - accuracy: 0.7708\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.6206 - accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2c64af8250>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CNN, at the end 5 epochs, accuracy was at around 71.02% which is a significant improvement over ANN. CNN's are best for image classification and gives superb accuracy. Also computation is much less compared to simple ANN as maxpooling reduces the image dimensions while still preserving the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.9540 - accuracy: 0.6917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9539900422096252, 0.6916999816894531]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = [np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74      1000\n",
      "           1       0.85      0.75      0.80      1000\n",
      "           2       0.63      0.56      0.59      1000\n",
      "           3       0.51      0.49      0.50      1000\n",
      "           4       0.63      0.62      0.63      1000\n",
      "           5       0.57      0.66      0.61      1000\n",
      "           6       0.76      0.77      0.76      1000\n",
      "           7       0.74      0.76      0.75      1000\n",
      "           8       0.84      0.73      0.78      1000\n",
      "           9       0.68      0.84      0.75      1000\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.70      0.69      0.69     10000\n",
      "weighted avg       0.70      0.69      0.69     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
